#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VLA Inference Server - GPU ì„œë²„ì—ì„œ ì‹¤í–‰

ZeroMQ REP ì†Œì¼“ì„ í†µí•´ ë¡œë´‡ laptopìœ¼ë¡œë¶€í„° ê´€ì¸¡ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ê³ ,
VLA ëª¨ë¸(SmolVLA, Pi0, GROOT N1.5, FMVLA) ì¶”ë¡  ê²°ê³¼(action)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python vla_inference_server.py \
        --policy_path ~/OpenArm0.3_data/checkpoints/smolvla_openarm_16dim/pretrained_model \
        --port 5555 \
        --model_type smolvla \
        --debug

Author: Antigravity Assistant
Date: 2026-02-09
"""

import argparse
import time
import sys
import threading
from typing import Dict, Any, Optional

import zmq
import msgpack
import numpy as np


class VLAInferenceServer:
    """VLA ëª¨ë¸ ì¶”ë¡  ì„œë²„ (ZeroMQ REP íŒ¨í„´)"""
    
    def __init__(
        self,
            policy_path: str,
        port: int = 5555,
        device: str = 'cuda',
        image_size: int = 256,
        model_type: str = 'smolvla',
        fmvla_sd3_model_path: str = "stabilityai/stable-diffusion-3-medium-diffusers",
        fmvla_lora_weights_path: Optional[str] = None,
        fmvla_lora_scale: float = 1.0,
        fmvla_enable_sd3_cpu_offload: bool = True,
        fmvla_precomputed_dir: Optional[str] = None,
        fmvla_precomputed_only: Optional[bool] = None,
        fmvla_chunk_size: Optional[int] = None,
        fmvla_n_action_steps: Optional[int] = None,
        fmvla_hold_on_chunk_boundary: bool = True,
        fmvla_hold_max_sec: float = 0.0,
    ):
        """
        Args:
            policy_path: ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            port: ZeroMQ ì„œë²„ í¬íŠ¸ (localhostì—ì„œë§Œ ë°”ì¸ë”©)
            device: GPU ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cuda:0' ë“±)
            image_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’ 256x256)
            model_type: ëª¨ë¸ íƒ€ì… ('smolvla', 'pi0', 'groot', ë˜ëŠ” 'fmvla')
        """
        self.device = device
        self.port = port
        self.image_size = image_size
        self.model_type = model_type
        self.fmvla_sd3_model_path = fmvla_sd3_model_path
        self.fmvla_lora_weights_path = fmvla_lora_weights_path
        self.fmvla_lora_scale = fmvla_lora_scale
        self.fmvla_enable_sd3_cpu_offload = fmvla_enable_sd3_cpu_offload
        self.fmvla_precomputed_dir = fmvla_precomputed_dir
        self.fmvla_precomputed_only = fmvla_precomputed_only
        self.fmvla_chunk_size = fmvla_chunk_size
        self.fmvla_n_action_steps = fmvla_n_action_steps
        self.fmvla_hold_on_chunk_boundary = fmvla_hold_on_chunk_boundary
        self.fmvla_hold_max_sec = fmvla_hold_max_sec
        self.debug = False
        self.policy = None
        self.preprocessor = None
        self.postprocessor = None

        # FMVLA hold-last-action state (used only when model_type='fmvla')
        self._fmvla_lock = threading.Lock()
        self._fmvla_worker_running = False
        self._fmvla_worker_thread = None
        self._fmvla_pending_first_action = None
        self._fmvla_pending_error = None
        self._fmvla_last_action = None
        self._fmvla_hold_start_ts = None
        self._fmvla_last_hold_log_ts = 0.0
        self._fmvla_last_hold_warn_ts = 0.0
        
        # ZeroMQ ì„¤ì •
        self._setup_zmq()
        
        # SmolVLA ì •ì±… ë¡œë“œ
        self._load_policy(policy_path)
        
        # í†µê³„
        self.inference_count = 0
        self.total_inference_time = 0.0
        
        self._print_banner()
    
    def _setup_zmq(self):
        """ZeroMQ REP ì†Œì¼“ ì„¤ì •"""
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.REP)
        # localhostì—ì„œë§Œ ë°”ì¸ë”© (SSH í„°ë„ì„ í†µí•´ ì ‘ê·¼)
        self.socket.bind(f"tcp://localhost:{self.port}")
        print(f"ğŸ”Œ ZeroMQ REP ì†Œì¼“ ë°”ì¸ë”©: tcp://localhost:{self.port}")
    
    def _load_policy(self, policy_path: str):
        """ëª¨ë¸ ì •ì±… ë¡œë“œ"""
        print(f"\nğŸ”„ {self.model_type} ì •ì±… ë¡œë”© ì¤‘...")
        print(f"   ê²½ë¡œ: {policy_path}")
        
        try:
            import torch
            from lerobot.policies.factory import make_pre_post_processors
            
            # ì •ì±… ë¡œë“œ
            if self.model_type == 'smolvla':
                from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy
                self.policy = SmolVLAPolicy.from_pretrained(policy_path)
            elif self.model_type == 'pi0':
                from lerobot.policies.pi0.modeling_pi0 import PI0Policy
                self.policy = PI0Policy.from_pretrained(policy_path)
            elif self.model_type == 'groot':
                from lerobot.policies.groot.modeling_groot import GrootPolicy
                self.policy = GrootPolicy.from_pretrained(policy_path)
            elif self.model_type == 'fmvla':
                # Register FMVLA processor step (fmvla_task_processor) before loading
                import lerobot.policies.fmvla.processor_fmvla  # noqa: F401
                from lerobot.configs.policies import PreTrainedConfig
                from lerobot.policies.fmvla.modeling_fmvla import FMVLA_Policy

                cli_overrides = ["--random_init_gemma_expert=false"]
                if self.fmvla_precomputed_dir:
                    cli_overrides.append(f"--precomputed_dir={self.fmvla_precomputed_dir}")
                else:
                    cli_overrides.append("--precomputed_dir=null")
                if self.fmvla_chunk_size is not None:
                    cli_overrides.append(f"--chunk_size={self.fmvla_chunk_size}")
                if self.fmvla_n_action_steps is not None:
                    cli_overrides.append(f"--n_action_steps={self.fmvla_n_action_steps}")
                if self.fmvla_lora_weights_path:
                    cli_overrides.append(f"--lora_weights_path={self.fmvla_lora_weights_path}")
                cli_overrides.append(f"--lora_scale={self.fmvla_lora_scale}")

                fmvla_config = PreTrainedConfig.from_pretrained(
                    policy_path,
                    cli_overrides=cli_overrides,
                )

                self.policy = FMVLA_Policy.from_pretrained(
                    pretrained_name_or_path=policy_path,
                    config=fmvla_config,
                    sd3_model_path=self.fmvla_sd3_model_path,
                    enable_sd3_cpu_offload=self.fmvla_enable_sd3_cpu_offload,
                    lora_weights_path=self.fmvla_lora_weights_path,
                    lora_scale=self.fmvla_lora_scale,
                    precomputed_dir=self.fmvla_precomputed_dir,
                    precomputed_only=self.fmvla_precomputed_only,
                )
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
            self.policy.to(self.device)
            self.policy.eval()
            
            # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ê¸° ìƒì„±
            preprocessor_overrides = {}
            if self.model_type == "fmvla":
                # Ensure runtime device matches CLI device argument.
                preprocessor_overrides["device_processor"] = {"device": str(self.device)}

            self.preprocessor, self.postprocessor = make_pre_post_processors(
                self.policy.config,
                pretrained_path=policy_path,
                preprocessor_overrides=preprocessor_overrides,
            )
            
            print(f"âœ… ì •ì±… ë¡œë“œ ì™„ë£Œ!")
            print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
            input_shapes = getattr(self.policy.config, 'input_shapes', {})
            output_shapes = getattr(self.policy.config, 'output_shapes', {})
            print(f"   State dim: {input_shapes.get('observation.state', 'N/A')}")
            print(f"   Action dim: {output_shapes.get('action', 'N/A')}")
            if self.model_type == 'fmvla' and self.fmvla_hold_on_chunk_boundary:
                hold_sec = "infinite" if self.fmvla_hold_max_sec <= 0 else f"{self.fmvla_hold_max_sec:.2f}s"
                print(f"   FMVLA hold-last-action: enabled (max hold: {hold_sec})")
            
        except ImportError as e:
            print(f"âŒ LeRobot íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
            print("   pip install lerobot==0.4.3 ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ ì •ì±… ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    def _print_banner(self):
        """ì„œë²„ ì‹œì‘ ë°°ë„ˆ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("  ğŸ¤– VLA Inference Server")
        print("=" * 60)
        print(f"  ğŸ“¡ ZeroMQ í¬íŠ¸: localhost:{self.port}")
        print(f"  ğŸ® ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  ğŸ–¼ï¸  ì´ë¯¸ì§€ í¬ê¸°: {self.image_size}x{self.image_size}")
        print("=" * 60)
        print("\nâ³ í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ëŒ€ê¸° ì¤‘...\n")
    
    def _parse_request(self, raw_data: bytes) -> Dict[str, Any]:
        """ìš”ì²­ ë°ì´í„° íŒŒì‹±"""
        data = msgpack.unpackb(raw_data, raw=False)
        
        # ì´ë¯¸ì§€ ë³µì› (bytes â†’ numpy array)
        images = {}
        # flat key êµ¬ì¡° ì²˜ë¦¬ (observation.images.camera1 ë“±)
        for key, value in data.items():
            if key.startswith('observation.images.'):
                img_bytes = value
                img_array = np.frombuffer(img_bytes, dtype=np.uint8)
                img_array = img_array.reshape(self.image_size, self.image_size, 3)
                images[key] = img_array
        
        # ì´ì „ 'images' ì¤‘ì²© êµ¬ì¡°ë„ í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ (ì„ íƒì‚¬í•­)
        if 'images' in data:
            for key, img_bytes in data['images'].items():
                img_array = np.frombuffer(img_bytes, dtype=np.uint8)
                img_array = img_array.reshape(self.image_size, self.image_size, 3)
                # ë§Œì•½ keyê°€ observation.images.ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ë¶™ì—¬ì¤Œ (ê¸°ì¡´ ë¡œì§ í˜¸í™˜)
                full_key = key if key.startswith('observation.images.') else f'observation.images.{key}'
                images[full_key] = img_array
        
        # ìƒíƒœ ë²¡í„°
        # observation.state í‚¤ë¡œ ì§ì ‘ ë“¤ì–´ì˜¬ ìˆ˜ë„ ìˆê³ , 'state' í‚¤ë¡œ ë“¤ì–´ì˜¬ ìˆ˜ë„ ìˆìŒ
        if 'observation.state' in data:
            state = np.array(data['observation.state'], dtype=np.float32)
        else:
            state = np.array(data.get('state', []), dtype=np.float32)
        
        # íƒœìŠ¤í¬ ì„¤ëª…
        task = data.get('task', 'manipulation task')
        
        return {
            'images': images,
            'state': state,
            'task': task,
        }
    
    def _prepare_batch(self, parsed_data: Dict[str, Any]) -> Dict[str, Any]:
        """ìš”ì²­ ë°ì´í„°ë¥¼ ì •ì±… ì…ë ¥ ë°°ì¹˜ë¡œ ë³€í™˜"""
        import torch
        
        images = parsed_data['images']
        state = parsed_data['state']
        task = parsed_data['task']
        
        # 1. ìƒíƒœ(State) ì²˜ë¦¬: (dim,) -> (1, dim) Tensor
        state_tensor = torch.from_numpy(state).float().to(self.device).unsqueeze(0)
        
        # ê´€ì¸¡ê°’ êµ¬ì„±
        observation = {
            'observation.state': state_tensor,
            'task': task,  # ë¬¸ìì—´ì€ ê·¸ëŒ€ë¡œ (ë°°ì¹˜ ì²˜ë¦¬ëŠ” ë‚´ë¶€ì—ì„œ)
        }
        
        # 2. ì´ë¯¸ì§€ ì²˜ë¦¬: (H, W, C) -> (1, C, H, W) Tensor + 0-1 ì •ê·œí™”
        # images ë”•ì…”ë„ˆë¦¬ëŠ” ì´ë¯¸ 'observation.images.cameraX' í˜•íƒœì˜ í‚¤ë¥¼ ê°€ì§
        for key, img_array in images.items():
            # numpy (H, W, C) -> torch (C, H, W)
            # Make copy to ensure writable memory (fixes UserWarning)
            img_tensor = torch.from_numpy(img_array.copy()).permute(2, 0, 1).float() / 255.0
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (1, C, H, W)
            img_tensor = img_tensor.unsqueeze(0).to(self.device)
            # ê´€ì¸¡ê°’ì— ì¶”ê°€ (í‚¤ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            observation[key] = img_tensor
        
        # ì „ì²˜ë¦¬ (ì´ë¯¸ ë°°ì¹˜í™”ëœ Tensorê°€ ë“¤ì–´ì˜´)
        # ì¤‘ìš”: smolvla_inference_node.py (ë¡œì»¬ ë…¸ë“œ)ì™€ ë™ì¼í•˜ê²Œ ì „ì²˜ë¦¬ê¸° í†µê³¼
        if self.preprocessor:
             batch = self.preprocessor(observation)
        else:
             batch = observation
            
        return batch

    def _finalize_action(self, action):
        """ì •ì±… ì¶œë ¥ action í›„ì²˜ë¦¬ ë° numpy ë³€í™˜"""
        import torch

        # [Emergency Fix] Action ì°¨ì› ë¶ˆì¼ì¹˜ í•´ê²° (32 -> 16)
        # PI0/FMVLA max_action_dim=32 ì¼€ì´ìŠ¤ì—ì„œ OpenArm ì œì–´ ì°¨ì›(16)ìœ¼ë¡œ ë§ì¶¤
        if hasattr(action, "shape") and action.shape[-1] == 32:
            action = action[..., :16]

        # í›„ì²˜ë¦¬ (unnormalize ë“±)
        if self.postprocessor:
            action = self.postprocessor(action)
        
        # numpy ë³€í™˜
        if isinstance(action, torch.Tensor):
            action = action.squeeze().cpu().numpy()
        else:
            action = np.array(action).squeeze()
        
        return action

    def _fmvla_queue_len(self) -> int:
        """FMVLA ë‚´ë¶€ action queue ê¸¸ì´ ë°˜í™˜"""
        queue = getattr(self.policy, "_action_queue", None)
        if queue is None:
            return 0
        return len(queue)

    def _set_fmvla_last_action(self, action_np: np.ndarray):
        """FMVLA holdìš© ë§ˆì§€ë§‰ action ì €ì¥"""
        self._fmvla_last_action = np.array(action_np, dtype=np.float32, copy=True)

    def _start_fmvla_chunk_worker(self, batch: Dict[str, Any]):
        """FMVLA ë‹¤ìŒ chunk ë¹„ë™ê¸° ìƒì„± ì‹œì‘"""
        if self._fmvla_worker_running:
            return

        self._fmvla_worker_running = True

        def _worker():
            import torch
            try:
                with torch.no_grad():
                    raw_action = self.policy.select_action(batch)
                next_action = self._finalize_action(raw_action)
                with self._fmvla_lock:
                    self._fmvla_pending_first_action = np.array(next_action, dtype=np.float32, copy=True)
                    self._fmvla_pending_error = None
                    self._fmvla_worker_running = False
            except Exception as worker_error:  # pragma: no cover - runtime safety path
                with self._fmvla_lock:
                    self._fmvla_pending_error = str(worker_error)
                    self._fmvla_worker_running = False

        self._fmvla_worker_thread = threading.Thread(target=_worker, daemon=True)
        self._fmvla_worker_thread.start()

    def _run_inference_fmvla_hold(self, parsed_data: Dict[str, Any]) -> np.ndarray:
        """
        FMVLA ì „ìš© hold-last-action ì¶”ë¡ :
        - queueì— actionì´ ë‚¨ì•„ìˆìœ¼ë©´ ì¦‰ì‹œ ì†Œë¹„
        - queueê°€ ë¹„ë©´ ë§ˆì§€ë§‰ action ìœ ì§€ + ë‹¤ìŒ chunk ë¹„ë™ê¸° ìƒì„±
        """
        import torch

        mode = None
        hold_action = None
        pending_action = None

        with self._fmvla_lock:
            if self._fmvla_pending_error:
                # ì•ˆì „ ìš°ì„ : ì˜¤ë¥˜ê°€ ë‚˜ë„ ì¦‰ì‹œ ì‹¤íŒ¨ ì‘ë‹µ ëŒ€ì‹  ë§ˆì§€ë§‰ action hold ìœ ì§€
                print(f"âš  FMVLA async chunk ìƒì„± ì˜¤ë¥˜: {self._fmvla_pending_error}")
                self._fmvla_pending_error = None

            if self._fmvla_pending_first_action is not None:
                pending_action = self._fmvla_pending_first_action
                self._fmvla_pending_first_action = None
                self._fmvla_hold_start_ts = None
                mode = "use_pending_first_action"
            elif self._fmvla_worker_running:
                hold_action = None if self._fmvla_last_action is None else self._fmvla_last_action.copy()
                mode = "hold_while_worker_running"
            else:
                queue_len = self._fmvla_queue_len()
                if queue_len > 0:
                    mode = "consume_from_queue"
                elif self._fmvla_last_action is None:
                    # ì²« ìš”ì²­ì—ëŠ” holdí•  actionì´ ì—†ìœ¼ë¯€ë¡œ ë™ê¸° 1íšŒ ë¶€íŠ¸ìŠ¤íŠ¸ë©
                    mode = "bootstrap_sync_first_action"
                else:
                    mode = "start_worker_and_hold"

        if mode == "use_pending_first_action":
            self._set_fmvla_last_action(pending_action)
            if self.debug:
                print("â–¶ FMVLA async chunk ì¤€ë¹„ ì™„ë£Œ: ìƒˆ chunk action ì¬ê°œ")
            return pending_action

        if mode == "consume_from_queue":
            batch = self._prepare_batch(parsed_data)
            with torch.no_grad():
                raw_action = self.policy.select_action(batch)
            action = self._finalize_action(raw_action)
            with self._fmvla_lock:
                self._set_fmvla_last_action(action)
            return action

        if mode == "bootstrap_sync_first_action":
            batch = self._prepare_batch(parsed_data)
            with torch.no_grad():
                raw_action = self.policy.select_action(batch)
            action = self._finalize_action(raw_action)
            with self._fmvla_lock:
                self._set_fmvla_last_action(action)
            if self.debug:
                print("â–¶ FMVLA ì²« action ë™ê¸° ë¶€íŠ¸ìŠ¤íŠ¸ë© ì™„ë£Œ")
            return action

        if mode == "start_worker_and_hold":
            batch = self._prepare_batch(parsed_data)
            with self._fmvla_lock:
                # lock ì¬ì§„ì… ì‹œì ì—ì„œ ìƒíƒœ ì¬í™•ì¸
                if self._fmvla_pending_first_action is not None:
                    pending_action = self._fmvla_pending_first_action
                    self._fmvla_pending_first_action = None
                    self._fmvla_hold_start_ts = None
                else:
                    if not self._fmvla_worker_running:
                        self._start_fmvla_chunk_worker(batch)
                        self._fmvla_hold_start_ts = time.time()
                    hold_action = self._fmvla_last_action.copy()

            if pending_action is not None:
                self._set_fmvla_last_action(pending_action)
                return pending_action

            now = time.time()
            hold_elapsed = 0.0
            if self._fmvla_hold_start_ts is not None:
                hold_elapsed = now - self._fmvla_hold_start_ts

            if self.debug and (now - self._fmvla_last_hold_log_ts) >= 1.0:
                print(f"â¸ FMVLA hold-last-action ìœ ì§€ ì¤‘... ({hold_elapsed:.2f}s)")
                self._fmvla_last_hold_log_ts = now

            if self.fmvla_hold_max_sec > 0 and hold_elapsed > self.fmvla_hold_max_sec:
                if (now - self._fmvla_last_hold_warn_ts) >= 5.0:
                    print(
                        f"âš  FMVLA hold ì‹œê°„ì´ {self.fmvla_hold_max_sec:.2f}së¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. "
                        "ì„¤ì •ìƒ ë¬´ê¸°í•œ holdê°€ ì•„ë‹ˆë¯€ë¡œ ìƒíƒœë¥¼ ì ê²€í•˜ì„¸ìš”."
                    )
                    self._fmvla_last_hold_warn_ts = now

            return hold_action

        # mode == "hold_while_worker_running"
        if hold_action is None:
            raise RuntimeError("FMVLA hold ìƒíƒœì—ì„œ last_actionì´ ì—†ìŠµë‹ˆë‹¤. ì´ˆê¸°í™” ê²½ë¡œë¥¼ ì ê²€í•˜ì„¸ìš”.")

        now = time.time()
        if self._fmvla_hold_start_ts is not None:
            hold_elapsed = now - self._fmvla_hold_start_ts
        else:
            hold_elapsed = 0.0

        if self.debug and (now - self._fmvla_last_hold_log_ts) >= 1.0:
            print(f"â¸ FMVLA worker ì‹¤í–‰ ì¤‘, hold-last-action ìœ ì§€... ({hold_elapsed:.2f}s)")
            self._fmvla_last_hold_log_ts = now

        return hold_action

    def _run_inference(self, parsed_data: Dict[str, Any]) -> np.ndarray:
        """VLA ì¶”ë¡  ì‹¤í–‰"""
        import torch

        # FMVLA ì „ìš©: queue ê²½ê³„ hold-last-action ëª¨ë“œ
        if self.model_type == "fmvla" and self.fmvla_hold_on_chunk_boundary:
            return self._run_inference_fmvla_hold(parsed_data)

        # ê¸°ë³¸(ê¸°ì¡´) ë™ì‘: ìš”ì²­ë§ˆë‹¤ ì¦‰ì‹œ select_action
        batch = self._prepare_batch(parsed_data)

        with torch.no_grad():
            raw_action = self.policy.select_action(batch)

        return self._finalize_action(raw_action)
    
    def _send_response(self, action: np.ndarray, inference_time: float, status: str = 'ok'):
        """ì‘ë‹µ ì „ì†¡"""
        response = {
            'action': action.tolist() if action is not None else [],
            'inference_time_ms': inference_time * 1000,
            'status': status,
        }
        self.socket.send(msgpack.packb(response))
    
    def _send_error(self, message: str):
        """ì—ëŸ¬ ì‘ë‹µ ì „ì†¡"""
        response = {
            'action': [],
            'inference_time_ms': 0,
            'status': 'error',
            'message': message,
        }
        self.socket.send(msgpack.packb(response))
    
    def run(self, debug: bool = False):
        """ë©”ì¸ ì„œë²„ ë£¨í”„"""
        self.debug = debug
        print("ğŸš€ ì„œë²„ ë£¨í”„ ì‹œì‘!")
        
        while True:
            try:
                # 1. ìš”ì²­ ìˆ˜ì‹ 
                raw_data = self.socket.recv()
                recv_time = time.time()
                
                # 2. ë°ì´í„° íŒŒì‹±
                parsed_data = self._parse_request(raw_data)
                
                if debug:
                    print(f"\nğŸ“¥ ìš”ì²­ ìˆ˜ì‹ :")
                    print(f"   ì´ë¯¸ì§€: {list(parsed_data['images'].keys())}")
                    print(f"   ìƒíƒœ: shape={parsed_data['state'].shape}")
                    print(f"   íƒœìŠ¤í¬: '{parsed_data['task'][:50]}...'")
                
                # 3. ì¶”ë¡  ì‹¤í–‰
                action = self._run_inference(parsed_data)
                inference_time = time.time() - recv_time
                
                # 4. ì‘ë‹µ ì „ì†¡
                self._send_response(action, inference_time)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.inference_count += 1
                self.total_inference_time += inference_time
                
                if debug:
                    print(f"ğŸ“¤ ì‘ë‹µ ì „ì†¡:")
                    print(f"   ì•¡ì…˜: {action[:4]}... (shape={action.shape})")
                    print(f"   ì¶”ë¡  ì‹œê°„: {inference_time * 1000:.1f}ms")
                
                # ì£¼ê¸°ì  í†µê³„ ì¶œë ¥ (50íšŒë§ˆë‹¤)
                if self.inference_count % 50 == 0:
                    avg_time = self.total_inference_time / self.inference_count * 1000
                    print(f"\nğŸ“Š í†µê³„: {self.inference_count}íšŒ ì¶”ë¡  ì™„ë£Œ, "
                          f"í‰ê·  {avg_time:.1f}ms/ì¶”ë¡ ")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ìš”ì²­ (Ctrl+C)")
                break
                
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                self._send_error(str(e))
        
        # ì •ë¦¬
        self._cleanup()
    
    def _cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print("\nğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        self.socket.close()
        self.context.term()
        print("âœ… ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")


def main():
    parser = argparse.ArgumentParser(
        description='VLA Inference Server - SmolVLA/Pi0/GROOT/FMVLA ì›ê²© ì¶”ë¡  ì„œë²„',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # SmolVLA ì‹¤í–‰ (ê¸°ë³¸ê°’)
  python vla_inference_server.py --policy_path ~/checkpoints/smolvla

  # Pi0 ì‹¤í–‰
  python vla_inference_server.py --policy_path ~/checkpoints/pi0 --model_type pi0

  # GROOT N1.5 ì‹¤í–‰
  python vla_inference_server.py --policy_path ~/checkpoints/groot --model_type groot

  # FMVLA ì‹¤í–‰ (ì „ìš© í™˜ê²½ vla_server_fmvla ê¶Œì¥)
  python vla_inference_server.py --policy_path ~/checkpoints/fmvla/pretrained_model --model_type fmvla

  # ë””ë²„ê·¸ ëª¨ë“œ + ë‹¤ë¥¸ í¬íŠ¸
  python vla_inference_server.py --policy_path ~/checkpoints/smolvla --debug --port 5556
        """
    )
    
    parser.add_argument(
        '--policy_path',
        type=str,
        required=True,
        help='ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (í•„ìˆ˜)'
    )
    parser.add_argument(
        '--port',
        type=int,
        default=5555,
        help='ZeroMQ ì„œë²„ í¬íŠ¸ (ê¸°ë³¸ê°’: 5555)'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda',
        help='GPU ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: cuda)'
    )
    parser.add_argument(
        '--image_size',
        type=int,
        default=256,
        help='ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 256)'
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ìƒì„¸ ë¡œê·¸ ì¶œë ¥)'
    )
    parser.add_argument(
        '--model_type',
        type=str,
        default='smolvla',
        choices=['smolvla', 'pi0', 'groot', 'fmvla'],
        help="ëª¨ë¸ íƒ€ì… ì„ íƒ: 'smolvla', 'pi0', 'groot', ë˜ëŠ” 'fmvla' (ê¸°ë³¸ê°’: smolvla)"
    )
    parser.add_argument(
        '--fmvla_sd3_model_path',
        type=str,
        default='stabilityai/stable-diffusion-3-medium-diffusers',
        help='FMVLA ì „ìš©: SD3 ëª¨ë¸ ê²½ë¡œ ë˜ëŠ” HF repo id'
    )
    parser.add_argument(
        '--fmvla_lora_weights_path',
        type=str,
        default=None,
        help='FMVLA ì „ìš©: Vision Planner LoRA ê°€ì¤‘ì¹˜ ê²½ë¡œ'
    )
    parser.add_argument(
        '--fmvla_lora_scale',
        type=float,
        default=1.0,
        help='FMVLA ì „ìš©: LoRA scale (ê¸°ë³¸ê°’: 1.0)'
    )
    parser.add_argument(
        '--fmvla_enable_sd3_cpu_offload',
        action=argparse.BooleanOptionalAction,
        default=True,
        help='FMVLA ì „ìš©: SD3 CPU offload ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)'
    )
    parser.add_argument(
        '--fmvla_precomputed_dir',
        type=str,
        default=None,
        help='FMVLA ì „ìš©: precomputed í‚¤/ì„œë¸Œê³¨ ë””ë ‰í† ë¦¬'
    )
    parser.add_argument(
        '--fmvla_precomputed_only',
        dest='fmvla_precomputed_only',
        action='store_true',
        default=None,
        help='FMVLA ì „ìš©: precomputed-only ëª¨ë“œ ê°•ì œ í™œì„±í™”'
    )
    parser.add_argument(
        '--fmvla_no_precomputed_only',
        dest='fmvla_precomputed_only',
        action='store_false',
        help='FMVLA ì „ìš©: precomputed-only ëª¨ë“œ ë¹„í™œì„±í™”'
    )
    parser.add_argument(
        '--fmvla_chunk_size',
        type=int,
        default=None,
        help='FMVLA ì „ìš©: chunk_size override'
    )
    parser.add_argument(
        '--fmvla_n_action_steps',
        type=int,
        default=None,
        help='FMVLA ì „ìš©: n_action_steps override'
    )
    parser.add_argument(
        '--fmvla_hold_on_chunk_boundary',
        action=argparse.BooleanOptionalAction,
        default=True,
        help='FMVLA ì „ìš©: action queue ê²½ê³„ì—ì„œ hold-last-action ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)'
    )
    parser.add_argument(
        '--fmvla_hold_max_sec',
        type=float,
        default=0.0,
        help='FMVLA ì „ìš©: hold ìµœëŒ€ ì‹œê°„(ì´ˆ). 0 ì´í•˜ëŠ” ë¬´ê¸°í•œ hold (ê¸°ë³¸ê°’: 0.0)'
    )
    
    args = parser.parse_args()
    
    # ì„œë²„ ìƒì„± ë° ì‹¤í–‰
    server = VLAInferenceServer(
        policy_path=args.policy_path,
        port=args.port,
        device=args.device,
        image_size=args.image_size,
        model_type=args.model_type,
        fmvla_sd3_model_path=args.fmvla_sd3_model_path,
        fmvla_lora_weights_path=args.fmvla_lora_weights_path,
        fmvla_lora_scale=args.fmvla_lora_scale,
        fmvla_enable_sd3_cpu_offload=args.fmvla_enable_sd3_cpu_offload,
        fmvla_precomputed_dir=args.fmvla_precomputed_dir,
        fmvla_precomputed_only=args.fmvla_precomputed_only,
        fmvla_chunk_size=args.fmvla_chunk_size,
        fmvla_n_action_steps=args.fmvla_n_action_steps,
        fmvla_hold_on_chunk_boundary=args.fmvla_hold_on_chunk_boundary,
        fmvla_hold_max_sec=args.fmvla_hold_max_sec,
    )
    
    server.run(debug=args.debug)


if __name__ == '__main__':
    main()
