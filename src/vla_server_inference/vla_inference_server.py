#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VLA Inference Server - GPU ì„œë²„ì—ì„œ ì‹¤í–‰

ZeroMQ REP ì†Œì¼“ì„ í†µí•´ ë¡œë´‡ laptopìœ¼ë¡œë¶€í„° ê´€ì¸¡ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ê³ ,
SmolVLA ì¶”ë¡  ê²°ê³¼(action)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python vla_inference_server.py \
        --policy_path ~/OpenArm0.3_data/checkpoints/smolvla_openarm_16dim/pretrained_model \
        --port 5555 \
        --debug

Author: Antigravity Assistant
Date: 2026-02-09
"""

import argparse
import time
import sys
from typing import Dict, Any, Optional

import zmq
import msgpack
import numpy as np


class VLAInferenceServer:
    """VLA ëª¨ë¸ ì¶”ë¡  ì„œë²„ (ZeroMQ REP íŒ¨í„´)"""
    
    def __init__(
        self,
        policy_path: str,
        port: int = 5555,
        device: str = 'cuda',
        image_size: int = 256
    ):
        """
        Args:
            policy_path: SmolVLA ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            port: ZeroMQ ì„œë²„ í¬íŠ¸ (localhostì—ì„œë§Œ ë°”ì¸ë”©)
            device: GPU ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cuda:0' ë“±)
            image_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’ 256x256)
        """
        self.device = device
        self.port = port
        self.image_size = image_size
        self.policy = None
        self.preprocessor = None
        self.postprocessor = None
        
        # ZeroMQ ì„¤ì •
        self._setup_zmq()
        
        # SmolVLA ì •ì±… ë¡œë“œ
        self._load_policy(policy_path)
        
        # í†µê³„
        self.inference_count = 0
        self.total_inference_time = 0.0
        
        self._print_banner()
    
    def _setup_zmq(self):
        """ZeroMQ REP ì†Œì¼“ ì„¤ì •"""
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.REP)
        # localhostì—ì„œë§Œ ë°”ì¸ë”© (SSH í„°ë„ì„ í†µí•´ ì ‘ê·¼)
        self.socket.bind(f"tcp://localhost:{self.port}")
        print(f"ğŸ”Œ ZeroMQ REP ì†Œì¼“ ë°”ì¸ë”©: tcp://localhost:{self.port}")
    
    def _load_policy(self, policy_path: str):
        """SmolVLA ì •ì±… ë¡œë“œ"""
        print(f"\nğŸ”„ SmolVLA ì •ì±… ë¡œë”© ì¤‘...")
        print(f"   ê²½ë¡œ: {policy_path}")
        
        try:
            import torch
            from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy
            from lerobot.policies.factory import make_pre_post_processors
            
            # ì •ì±… ë¡œë“œ
            self.policy = SmolVLAPolicy.from_pretrained(policy_path)
            self.policy.to(self.device)
            self.policy.eval()
            
            # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ê¸° ìƒì„±
            self.preprocessor, self.postprocessor = make_pre_post_processors(
                self.policy.config,
                pretrained_path=policy_path
            )
            
            print(f"âœ… ì •ì±… ë¡œë“œ ì™„ë£Œ!")
            print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
            print(f"   State dim: {self.policy.config.input_shapes.get('observation.state', 'N/A')}")
            print(f"   Action dim: {self.policy.config.output_shapes.get('action', 'N/A')}")
            
        except ImportError as e:
            print(f"âŒ LeRobot íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
            print("   pip install lerobot==0.4.3 ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ ì •ì±… ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    def _print_banner(self):
        """ì„œë²„ ì‹œì‘ ë°°ë„ˆ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("  ğŸ¤– VLA Inference Server")
        print("=" * 60)
        print(f"  ğŸ“¡ ZeroMQ í¬íŠ¸: localhost:{self.port}")
        print(f"  ğŸ® ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  ğŸ–¼ï¸  ì´ë¯¸ì§€ í¬ê¸°: {self.image_size}x{self.image_size}")
        print("=" * 60)
        print("\nâ³ í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ëŒ€ê¸° ì¤‘...\n")
    
    def _parse_request(self, raw_data: bytes) -> Dict[str, Any]:
        """ìš”ì²­ ë°ì´í„° íŒŒì‹±"""
        data = msgpack.unpackb(raw_data, raw=False)
        
        # ì´ë¯¸ì§€ ë³µì› (bytes â†’ numpy array)
        images = {}
        for key, img_bytes in data.get('images', {}).items():
            img_array = np.frombuffer(img_bytes, dtype=np.uint8)
            img_array = img_array.reshape(self.image_size, self.image_size, 3)
            images[key] = img_array
        
        # ìƒíƒœ ë²¡í„°
        state = np.array(data.get('state', []), dtype=np.float32)
        
        # íƒœìŠ¤í¬ ì„¤ëª…
        task = data.get('task', 'manipulation task')
        
        return {
            'images': images,
            'state': state,
            'task': task,
        }
    
    def _run_inference(self, parsed_data: Dict[str, Any]) -> np.ndarray:
        """SmolVLA ì¶”ë¡  ì‹¤í–‰"""
        import torch
        
        images = parsed_data['images']
        state = parsed_data['state']
        task = parsed_data['task']
        
        # ê´€ì¸¡ êµ¬ì„± (SmolVLA 16-dim í˜•ì‹)
        observation = {
            'observation.state': state,
            'task': task,
        }
        
        # ì¹´ë©”ë¼ ì´ë¯¸ì§€ ì¶”ê°€
        camera_mapping = {
            'top': 'observation.images.top',
            'wrist_left': 'observation.images.wrist_left',
            'wrist_right': 'observation.images.wrist_right',
        }
        
        for src_key, dst_key in camera_mapping.items():
            if src_key in images:
                observation[dst_key] = images[src_key]
        
        # ì „ì²˜ë¦¬
        batch = self.preprocessor(observation)
        
        # ì¶”ë¡ 
        with torch.no_grad():
            action = self.policy.select_action(batch)
        
        # í›„ì²˜ë¦¬
        action = self.postprocessor(action)
        
        # numpy ë³€í™˜
        if isinstance(action, torch.Tensor):
            action_np = action.squeeze().cpu().numpy()
        else:
            action_np = np.array(action).squeeze()
        
        return action_np
    
    def _send_response(self, action: np.ndarray, inference_time: float, status: str = 'ok'):
        """ì‘ë‹µ ì „ì†¡"""
        response = {
            'action': action.tolist() if action is not None else [],
            'inference_time_ms': inference_time * 1000,
            'status': status,
        }
        self.socket.send(msgpack.packb(response))
    
    def _send_error(self, message: str):
        """ì—ëŸ¬ ì‘ë‹µ ì „ì†¡"""
        response = {
            'action': [],
            'inference_time_ms': 0,
            'status': 'error',
            'message': message,
        }
        self.socket.send(msgpack.packb(response))
    
    def run(self, debug: bool = False):
        """ë©”ì¸ ì„œë²„ ë£¨í”„"""
        print("ğŸš€ ì„œë²„ ë£¨í”„ ì‹œì‘!")
        
        while True:
            try:
                # 1. ìš”ì²­ ìˆ˜ì‹ 
                raw_data = self.socket.recv()
                recv_time = time.time()
                
                # 2. ë°ì´í„° íŒŒì‹±
                parsed_data = self._parse_request(raw_data)
                
                if debug:
                    print(f"\nğŸ“¥ ìš”ì²­ ìˆ˜ì‹ :")
                    print(f"   ì´ë¯¸ì§€: {list(parsed_data['images'].keys())}")
                    print(f"   ìƒíƒœ: shape={parsed_data['state'].shape}")
                    print(f"   íƒœìŠ¤í¬: '{parsed_data['task'][:50]}...'")
                
                # 3. ì¶”ë¡  ì‹¤í–‰
                action = self._run_inference(parsed_data)
                inference_time = time.time() - recv_time
                
                # 4. ì‘ë‹µ ì „ì†¡
                self._send_response(action, inference_time)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.inference_count += 1
                self.total_inference_time += inference_time
                
                if debug:
                    print(f"ğŸ“¤ ì‘ë‹µ ì „ì†¡:")
                    print(f"   ì•¡ì…˜: {action[:4]}... (shape={action.shape})")
                    print(f"   ì¶”ë¡  ì‹œê°„: {inference_time * 1000:.1f}ms")
                
                # ì£¼ê¸°ì  í†µê³„ ì¶œë ¥ (50íšŒë§ˆë‹¤)
                if self.inference_count % 50 == 0:
                    avg_time = self.total_inference_time / self.inference_count * 1000
                    print(f"\nğŸ“Š í†µê³„: {self.inference_count}íšŒ ì¶”ë¡  ì™„ë£Œ, "
                          f"í‰ê·  {avg_time:.1f}ms/ì¶”ë¡ ")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ìš”ì²­ (Ctrl+C)")
                break
                
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                self._send_error(str(e))
        
        # ì •ë¦¬
        self._cleanup()
    
    def _cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print("\nğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        self.socket.close()
        self.context.term()
        print("âœ… ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")


def main():
    parser = argparse.ArgumentParser(
        description='VLA Inference Server - SmolVLA ì›ê²© ì¶”ë¡  ì„œë²„',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‹¤í–‰
  python vla_inference_server.py --policy_path ~/checkpoints/smolvla

  # ë””ë²„ê·¸ ëª¨ë“œ
  python vla_inference_server.py --policy_path ~/checkpoints/smolvla --debug

  # ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
  python vla_inference_server.py --policy_path ~/checkpoints/smolvla --port 5556
        """
    )
    
    parser.add_argument(
        '--policy_path',
        type=str,
        required=True,
        help='SmolVLA ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (í•„ìˆ˜)'
    )
    parser.add_argument(
        '--port',
        type=int,
        default=5555,
        help='ZeroMQ ì„œë²„ í¬íŠ¸ (ê¸°ë³¸ê°’: 5555)'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda',
        help='GPU ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: cuda)'
    )
    parser.add_argument(
        '--image_size',
        type=int,
        default=256,
        help='ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 256)'
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ìƒì„¸ ë¡œê·¸ ì¶œë ¥)'
    )
    
    args = parser.parse_args()
    
    # ì„œë²„ ìƒì„± ë° ì‹¤í–‰
    server = VLAInferenceServer(
        policy_path=args.policy_path,
        port=args.port,
        device=args.device,
        image_size=args.image_size
    )
    
    server.run(debug=args.debug)


if __name__ == '__main__':
    main()
