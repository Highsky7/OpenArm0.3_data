#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VLA Inference Server - GPU ì„œë²„ì—ì„œ ì‹¤í–‰

ZeroMQ REP ì†Œì¼“ì„ í†µí•´ ë¡œë´‡ laptopìœ¼ë¡œë¶€í„° ê´€ì¸¡ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ê³ ,
SmolVLA ì¶”ë¡  ê²°ê³¼(action)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python vla_inference_server.py \
        --policy_path ~/OpenArm0.3_data/checkpoints/smolvla_openarm_16dim/pretrained_model \
        --port 5555 \
        --debug

Author: Antigravity Assistant
Date: 2026-02-09
"""

import argparse
import time
import sys
from typing import Dict, Any, Optional

import zmq
import msgpack
import numpy as np


class VLAInferenceServer:
    """VLA ëª¨ë¸ ì¶”ë¡  ì„œë²„ (ZeroMQ REP íŒ¨í„´)"""
    
    def __init__(
        self,
        policy_path: str,
        port: int = 5555,
        device: str = 'cuda',
        image_size: int = 256
    ):
        """
        Args:
            policy_path: SmolVLA ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
            port: ZeroMQ ì„œë²„ í¬íŠ¸ (localhostì—ì„œë§Œ ë°”ì¸ë”©)
            device: GPU ë””ë°”ì´ìŠ¤ ('cuda' ë˜ëŠ” 'cuda:0' ë“±)
            image_size: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’ 256x256)
        """
        self.device = device
        self.port = port
        self.image_size = image_size
        self.policy = None
        self.preprocessor = None
        self.postprocessor = None
        
        # ZeroMQ ì„¤ì •
        self._setup_zmq()
        
        # SmolVLA ì •ì±… ë¡œë“œ
        self._load_policy(policy_path)
        
        # í†µê³„
        self.inference_count = 0
        self.total_inference_time = 0.0
        
        self._print_banner()
    
    def _setup_zmq(self):
        """ZeroMQ REP ì†Œì¼“ ì„¤ì •"""
        self.context = zmq.Context()
        self.socket = self.context.socket(zmq.REP)
        # localhostì—ì„œë§Œ ë°”ì¸ë”© (SSH í„°ë„ì„ í†µí•´ ì ‘ê·¼)
        self.socket.bind(f"tcp://localhost:{self.port}")
        print(f"ğŸ”Œ ZeroMQ REP ì†Œì¼“ ë°”ì¸ë”©: tcp://localhost:{self.port}")
    
    def _load_policy(self, policy_path: str):
        """SmolVLA ì •ì±… ë¡œë“œ"""
        print(f"\nğŸ”„ SmolVLA ì •ì±… ë¡œë”© ì¤‘...")
        print(f"   ê²½ë¡œ: {policy_path}")
        
        try:
            import torch
            from lerobot.policies.smolvla.modeling_smolvla import SmolVLAPolicy
            from lerobot.policies.factory import make_pre_post_processors
            
            # ì •ì±… ë¡œë“œ
            self.policy = SmolVLAPolicy.from_pretrained(policy_path)
            self.policy.to(self.device)
            self.policy.eval()
            
            # ì „ì²˜ë¦¬/í›„ì²˜ë¦¬ê¸° ìƒì„±
            self.preprocessor, self.postprocessor = make_pre_post_processors(
                self.policy.config,
                pretrained_path=policy_path
            )
            
            print(f"âœ… ì •ì±… ë¡œë“œ ì™„ë£Œ!")
            print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
            input_shapes = getattr(self.policy.config, 'input_shapes', {})
            output_shapes = getattr(self.policy.config, 'output_shapes', {})
            print(f"   State dim: {input_shapes.get('observation.state', 'N/A')}")
            print(f"   Action dim: {output_shapes.get('action', 'N/A')}")
            
        except ImportError as e:
            print(f"âŒ LeRobot íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ì‹¤íŒ¨: {e}")
            print("   pip install lerobot==0.4.3 ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            sys.exit(1)
        except Exception as e:
            print(f"âŒ ì •ì±… ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)
    
    def _print_banner(self):
        """ì„œë²„ ì‹œì‘ ë°°ë„ˆ ì¶œë ¥"""
        print("\n" + "=" * 60)
        print("  ğŸ¤– VLA Inference Server")
        print("=" * 60)
        print(f"  ğŸ“¡ ZeroMQ í¬íŠ¸: localhost:{self.port}")
        print(f"  ğŸ® ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"  ğŸ–¼ï¸  ì´ë¯¸ì§€ í¬ê¸°: {self.image_size}x{self.image_size}")
        print("=" * 60)
        print("\nâ³ í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ëŒ€ê¸° ì¤‘...\n")
    
    def _parse_request(self, raw_data: bytes) -> Dict[str, Any]:
        """ìš”ì²­ ë°ì´í„° íŒŒì‹±"""
        data = msgpack.unpackb(raw_data, raw=False)
        
        # ì´ë¯¸ì§€ ë³µì› (bytes â†’ numpy array)
        images = {}
        # flat key êµ¬ì¡° ì²˜ë¦¬ (observation.images.camera1 ë“±)
        for key, value in data.items():
            if key.startswith('observation.images.'):
                img_bytes = value
                img_array = np.frombuffer(img_bytes, dtype=np.uint8)
                img_array = img_array.reshape(self.image_size, self.image_size, 3)
                images[key] = img_array
        
        # ì´ì „ 'images' ì¤‘ì²© êµ¬ì¡°ë„ í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€ (ì„ íƒì‚¬í•­)
        if 'images' in data:
            for key, img_bytes in data['images'].items():
                img_array = np.frombuffer(img_bytes, dtype=np.uint8)
                img_array = img_array.reshape(self.image_size, self.image_size, 3)
                # ë§Œì•½ keyê°€ observation.images.ë¡œ ì‹œì‘í•˜ì§€ ì•Šìœ¼ë©´ ë¶™ì—¬ì¤Œ (ê¸°ì¡´ ë¡œì§ í˜¸í™˜)
                full_key = key if key.startswith('observation.images.') else f'observation.images.{key}'
                images[full_key] = img_array
        
        # ìƒíƒœ ë²¡í„°
        # observation.state í‚¤ë¡œ ì§ì ‘ ë“¤ì–´ì˜¬ ìˆ˜ë„ ìˆê³ , 'state' í‚¤ë¡œ ë“¤ì–´ì˜¬ ìˆ˜ë„ ìˆìŒ
        if 'observation.state' in data:
            state = np.array(data['observation.state'], dtype=np.float32)
        else:
            state = np.array(data.get('state', []), dtype=np.float32)
        
        # íƒœìŠ¤í¬ ì„¤ëª…
        task = data.get('task', 'manipulation task')
        
        return {
            'images': images,
            'state': state,
            'task': task,
        }
    
    def _run_inference(self, parsed_data: Dict[str, Any]) -> np.ndarray:
        """SmolVLA ì¶”ë¡  ì‹¤í–‰"""
        import torch
        
        images = parsed_data['images']
        state = parsed_data['state']
        task = parsed_data['task']
        
        # 1. ìƒíƒœ(State) ì²˜ë¦¬: (dim,) -> (1, dim) Tensor
        state_tensor = torch.from_numpy(state).float().to(self.device).unsqueeze(0)
        
        # ê´€ì¸¡ê°’ êµ¬ì„±
        observation = {
            'observation.state': state_tensor,
            'task': task,  # ë¬¸ìì—´ì€ ê·¸ëŒ€ë¡œ (ë°°ì¹˜ ì²˜ë¦¬ëŠ” ë‚´ë¶€ì—ì„œ)
        }
        
        # 2. ì´ë¯¸ì§€ ì²˜ë¦¬: (H, W, C) -> (1, C, H, W) Tensor + 0-1 ì •ê·œí™”
        # images ë”•ì…”ë„ˆë¦¬ëŠ” ì´ë¯¸ 'observation.images.cameraX' í˜•íƒœì˜ í‚¤ë¥¼ ê°€ì§
        for key, img_array in images.items():
            # numpy (H, W, C) -> torch (C, H, W)
            img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
            # ë°°ì¹˜ ì°¨ì› ì¶”ê°€: (1, C, H, W)
            img_tensor = img_tensor.unsqueeze(0).to(self.device)
            # ê´€ì¸¡ê°’ì— ì¶”ê°€ (í‚¤ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
            observation[key] = img_tensor
        
        # ì „ì²˜ë¦¬ (ì´ë¯¸ ë°°ì¹˜í™”ëœ Tensorê°€ ë“¤ì–´ì˜´)
        # ì£¼ì˜: LeRobotì˜ preprocessorëŠ” ë°ì´í„°ì…‹ ì•„ì´í…œ(dict)ì„ ê¸°ëŒ€í•  ìˆ˜ë„ ìˆìŒ
        # í•˜ì§€ë§Œ ì´ë¯¸ Tensorë¡œ ë³€í™˜í–ˆìœ¼ë¯€ë¡œ, ì—¬ê¸°ì„œëŠ” í•„ìš”í•œ ì¶”ê°€ ì •ê·œí™”ë§Œ ìˆ˜í–‰í•˜ê±°ë‚˜ íŒ¨ìŠ¤
        # ë§Œì•½ preprocessorê°€ Noneì´ê±°ë‚˜ Tensor ì…ë ¥ì„ ì²˜ë¦¬í•œë‹¤ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        if self.preprocessor:
             # preprocessorê°€ ë°°ì¹˜ ì…ë ¥ì„ ì²˜ë¦¬í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”.
             # ë³´í†µì€ ë°ì´í„°ì…‹ ì•„ì´í…œ(Unbatched) -> Batchedë¡œ ë³€í™˜í•˜ëŠ”ë°,
             # ì—¬ê¸°ì„œëŠ” ì§ì ‘ Batched Tensorë¥¼ ë§Œë“¤ì—ˆìœ¼ë¯€ë¡œ preprocessor í˜¸ì¶œ ë°©ì‹ì— ì£¼ì˜
             # ì¼ë‹¨ì€ policyì— ì§ì ‘ ë„£ê¸° ìœ„í•´ preprocessor í†µê³¼ (í•„ìš” ì‹œ)
             # í•˜ì§€ë§Œ VLA ëª¨ë¸ì€ ë³´í†µ normalizeë¥¼ ë‚´ë¶€ì—ì„œ í•˜ê±°ë‚˜ ì „ì²˜ë¦¬ê¸°ê°€ í•¨.
             # ìˆ˜ë™ìœ¼ë¡œ 0-1 ì •ê·œí™”í–ˆìœ¼ë‹ˆ, preprocessorê°€ ì¤‘ë³µ ì •ê·œí™”í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜.
             # *SmolVLA*ì˜ ê²½ìš° ì „ì²˜ë¦¬ê¸°ê°€ ë³µì¡í•  ìˆ˜ ìˆìŒ.
             # ì•ˆì „í•˜ê²ŒëŠ”: preprocessor í˜¸ì¶œ ì—†ì´ ì§ì ‘ í¬ë§·íŒ…í–ˆìœ¼ë¯€ë¡œ ë°”ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜,
             # preprocessorê°€ (C,H,W) ì…ë ¥ì„ ê¸°ëŒ€í•œë‹¤ë©´ ë°°ì¹˜ ì°¨ì› ì¶”ê°€ ì „ í˜¸ì¶œí•´ì•¼ í•¨.
             pass

        # SmolVLAì˜ ê²½ìš°, preprocessorê°€ í† í¬ë‚˜ì´ì§• ë“±ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í˜¸ì¶œ í•„ìš”.
        # ë‹¨, state/imageê°€ ì´ë¯¸ Tensorë¼ë©´?
        # -> SmolVLA preprocessor(normalization)ëŠ” ë³´í†µ ë°ì´í„°ì…‹ ë¡œë”ì—ì„œ ìˆ˜í–‰ë¨.
        # -> ì¶”ë¡  ì‹œì—ëŠ” ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” raw input(0-1 float)ì„ ë„£ì–´ì£¼ë©´ ë¨.
        
        # 3. ì¶”ë¡  ì‹¤í–‰
        # policy.select_actionì€ ë°°ì¹˜ë¥¼ ê¸°ëŒ€í•¨
        with torch.no_grad():
            # íƒœìŠ¤í¬ê°€ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë“¤ì–´ê°€ì•¼ í•¨ (ë°°ì¹˜ í¬ê¸° 1)
            # observation['task'] = [task] 
            # (SmolVLA ë‚´ë¶€ êµ¬í˜„ì— ë”°ë¼ ë‹¤ë¦„, ë³´í†µ í…ìŠ¤íŠ¸ëŠ” ë¦¬ìŠ¤íŠ¸)
            
            # ë§Œì•½ preprocessorë¥¼ ê¼­ ì¨ì•¼í•œë‹¤ë©´:
            # batch = self.preprocessor(observation) 
            # í•˜ì§€ë§Œ ìœ„ ì½”ë“œëŠ” ë°ì´í„°ì…‹ìš©ì¼ ìˆ˜ ìˆìŒ.
            
            # ì§ì ‘ êµ¬ì„±í•œ batch ì‚¬ìš©
            batch = observation
            
            # Action ì¶”ë¡ 
            action = self.policy.select_action(batch)
        
        # í›„ì²˜ë¦¬
        if self.postprocessor:
            action = self.postprocessor(action)
        
        # numpy ë³€í™˜
        if isinstance(action, torch.Tensor):
            action = action.squeeze().cpu().numpy()
        else:
            action = np.array(action).squeeze()
        
        return action
    
    def _send_response(self, action: np.ndarray, inference_time: float, status: str = 'ok'):
        """ì‘ë‹µ ì „ì†¡"""
        response = {
            'action': action.tolist() if action is not None else [],
            'inference_time_ms': inference_time * 1000,
            'status': status,
        }
        self.socket.send(msgpack.packb(response))
    
    def _send_error(self, message: str):
        """ì—ëŸ¬ ì‘ë‹µ ì „ì†¡"""
        response = {
            'action': [],
            'inference_time_ms': 0,
            'status': 'error',
            'message': message,
        }
        self.socket.send(msgpack.packb(response))
    
    def run(self, debug: bool = False):
        """ë©”ì¸ ì„œë²„ ë£¨í”„"""
        print("ğŸš€ ì„œë²„ ë£¨í”„ ì‹œì‘!")
        
        while True:
            try:
                # 1. ìš”ì²­ ìˆ˜ì‹ 
                raw_data = self.socket.recv()
                recv_time = time.time()
                
                # 2. ë°ì´í„° íŒŒì‹±
                parsed_data = self._parse_request(raw_data)
                
                if debug:
                    print(f"\nğŸ“¥ ìš”ì²­ ìˆ˜ì‹ :")
                    print(f"   ì´ë¯¸ì§€: {list(parsed_data['images'].keys())}")
                    print(f"   ìƒíƒœ: shape={parsed_data['state'].shape}")
                    print(f"   íƒœìŠ¤í¬: '{parsed_data['task'][:50]}...'")
                
                # 3. ì¶”ë¡  ì‹¤í–‰
                action = self._run_inference(parsed_data)
                inference_time = time.time() - recv_time
                
                # 4. ì‘ë‹µ ì „ì†¡
                self._send_response(action, inference_time)
                
                # í†µê³„ ì—…ë°ì´íŠ¸
                self.inference_count += 1
                self.total_inference_time += inference_time
                
                if debug:
                    print(f"ğŸ“¤ ì‘ë‹µ ì „ì†¡:")
                    print(f"   ì•¡ì…˜: {action[:4]}... (shape={action.shape})")
                    print(f"   ì¶”ë¡  ì‹œê°„: {inference_time * 1000:.1f}ms")
                
                # ì£¼ê¸°ì  í†µê³„ ì¶œë ¥ (50íšŒë§ˆë‹¤)
                if self.inference_count % 50 == 0:
                    avg_time = self.total_inference_time / self.inference_count * 1000
                    print(f"\nğŸ“Š í†µê³„: {self.inference_count}íšŒ ì¶”ë¡  ì™„ë£Œ, "
                          f"í‰ê·  {avg_time:.1f}ms/ì¶”ë¡ ")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ›‘ ì„œë²„ ì¢…ë£Œ ìš”ì²­ (Ctrl+C)")
                break
                
            except Exception as e:
                print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
                import traceback
                traceback.print_exc()
                self._send_error(str(e))
        
        # ì •ë¦¬
        self._cleanup()
    
    def _cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        print("\nğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...")
        self.socket.close()
        self.context.term()
        print("âœ… ì„œë²„ ì¢…ë£Œ ì™„ë£Œ")


def main():
    parser = argparse.ArgumentParser(
        description='VLA Inference Server - SmolVLA ì›ê²© ì¶”ë¡  ì„œë²„',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê¸°ë³¸ ì‹¤í–‰
  python vla_inference_server.py --policy_path ~/checkpoints/smolvla

  # ë””ë²„ê·¸ ëª¨ë“œ
  python vla_inference_server.py --policy_path ~/checkpoints/smolvla --debug

  # ë‹¤ë¥¸ í¬íŠ¸ ì‚¬ìš©
  python vla_inference_server.py --policy_path ~/checkpoints/smolvla --port 5556
        """
    )
    
    parser.add_argument(
        '--policy_path',
        type=str,
        required=True,
        help='SmolVLA ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (í•„ìˆ˜)'
    )
    parser.add_argument(
        '--port',
        type=int,
        default=5555,
        help='ZeroMQ ì„œë²„ í¬íŠ¸ (ê¸°ë³¸ê°’: 5555)'
    )
    parser.add_argument(
        '--device',
        type=str,
        default='cuda',
        help='GPU ë””ë°”ì´ìŠ¤ (ê¸°ë³¸ê°’: cuda)'
    )
    parser.add_argument(
        '--image_size',
        type=int,
        default=256,
        help='ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (ê¸°ë³¸ê°’: 256)'
    )
    parser.add_argument(
        '--debug',
        action='store_true',
        help='ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” (ìƒì„¸ ë¡œê·¸ ì¶œë ¥)'
    )
    
    args = parser.parse_args()
    
    # ì„œë²„ ìƒì„± ë° ì‹¤í–‰
    server = VLAInferenceServer(
        policy_path=args.policy_path,
        port=args.port,
        device=args.device,
        image_size=args.image_size
    )
    
    server.run(debug=args.debug)


if __name__ == '__main__':
    main()
